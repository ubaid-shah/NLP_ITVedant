{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "![](https://miro.medium.com/v2/resize:fit:1200/1*VT7AxioAGXplMe7RAEYfSA.png)\n",
        "Sentiment analysis for Twitter is a specific application of sentiment analysis that focuses on analyzing the sentiment expressed in tweets, which are short, often informal messages posted on the Twitter platform. This type of analysis is valuable for understanding public opinion, brand perception, and tracking trends in real-time.\n"
      ],
      "metadata": {
        "id": "KlR2oIwCiYm2"
      },
      "id": "KlR2oIwCiYm2"
    },
    {
      "cell_type": "markdown",
      "id": "8d049eaa",
      "metadata": {
        "id": "8d049eaa"
      },
      "source": [
        "### Context\n",
        "This is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 4 = positive) and they can be used to detect sentiment .\n",
        "\n",
        "#### Content\n",
        "It contains the following 6 fields:\n",
        "\n",
        "target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
        "\n",
        "ids: The id of the tweet ( 2087)\n",
        "\n",
        "date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
        "\n",
        "flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
        "\n",
        "user: the user that tweeted (robotickilldozr)\n",
        "\n",
        "text: the text of the tweet (Lyx is cool)\n",
        "\n",
        "#### Acknowledgements\n",
        "The official link regarding the dataset with resources about how it was generated is here\n",
        "The official paper detailing the approach is here\n",
        "\n",
        "Citation: Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(2009), p.12."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0S-g6VoqlbC4"
      },
      "id": "0S-g6VoqlbC4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "id": "8pwr-prBlYGr"
      },
      "id": "8pwr-prBlYGr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1175d0c5",
      "metadata": {
        "id": "1175d0c5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# plotting\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "# nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# sklearn\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8d5e2af",
      "metadata": {
        "id": "a8d5e2af"
      },
      "outputs": [],
      "source": [
        "# Importing the dataset\n",
        "DATASET_COLUMNS=['target','ids','date','flag','user','text']\n",
        "DATASET_ENCODING = \"ISO-8859-1\"\n",
        "df = pd.read_csv('/content/drive/MyDrive/DATA SETS/twitter.csv', encoding=DATASET_ENCODING,names=DATASET_COLUMNS)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90be540b",
      "metadata": {
        "id": "90be540b"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfaa9055",
      "metadata": {
        "id": "cfaa9055"
      },
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e552e68",
      "metadata": {
        "id": "9e552e68"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33a9f0ec",
      "metadata": {
        "id": "33a9f0ec"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d101937b",
      "metadata": {
        "id": "d101937b"
      },
      "outputs": [],
      "source": [
        "df['target'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b1f43d",
      "metadata": {
        "id": "11b1f43d"
      },
      "outputs": [],
      "source": [
        "df['target'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7be953d6",
      "metadata": {
        "id": "7be953d6"
      },
      "outputs": [],
      "source": [
        "dataset=df[['text','target']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d515d63",
      "metadata": {
        "id": "6d515d63"
      },
      "outputs": [],
      "source": [
        "dataset['target'] = dataset['target'].replace(4,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf9d3906",
      "metadata": {
        "id": "cf9d3906"
      },
      "outputs": [],
      "source": [
        "dataset['target'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb2b5e9f",
      "metadata": {
        "id": "bb2b5e9f"
      },
      "source": [
        "### Steps for Text Cleaning\n",
        "Text cleaning is a crucial preprocessing step in sentiment analysis to ensure that the text data is in a suitable format for analysis. Let's discuss each of the steps you've mentioned in detail:\n",
        "\n",
        "**Step 1: Remove HTML Tags**\n",
        "\n",
        "HTML tags are used to format text on web pages. When performing sentiment analysis on text data extracted from websites, it's essential to remove these tags as they don't contribute to the sentiment and can interfere with the analysis.\n",
        "\n",
        "**Step 2: Remove URLs**\n",
        "\n",
        "Uniform Resource Locators (URLs) are web addresses that often appear in text. They usually don't provide any meaningful sentiment information and can be removed to make the text more focused on the content itself.\n",
        "\n",
        "**Step 3: Handling Emojis**\n",
        "\n",
        "Emojis are graphical symbols that can convey emotions. You can choose to keep, remove, or replace emojis with their textual equivalents (e.g., converting ðŸ˜Š to \"smile\"). The choice depends on whether you want to incorporate emoji sentiment into your analysis.\n",
        "\n",
        "**Step 4: Chat word treatment**\n",
        "\n",
        "In social media and online conversations, people often use slang, abbreviations, and informal language. You may need to replace these with their standard equivalents. For example, \"u\" becomes \"you,\" \"gr8\" becomes \"great,\" and so on. This step helps standardize the text for analysis.\n",
        "\n",
        "**Step 5: Remove Punctuations**\n",
        "\n",
        "Punctuation marks (e.g., !, ?, .) don't typically carry sentiment information and can be removed. However, in some cases, you might want to keep certain punctuation marks to understand the sentiment better, such as exclamation points to identify excitement or question marks for uncertainty.\n",
        "\n",
        "**Step 6: Make Lower Case**\n",
        "\n",
        "Consistency is essential in text analysis. Converting all text to lowercase ensures that \"happy\" and \"Happy\" are treated as the same word, avoiding duplication and improving analysis accuracy.\n",
        "\n",
        "**Step 7: Spelling Correction**\n",
        "\n",
        "Correcting misspelled words is important to improve the quality of sentiment analysis. You can use spelling correction algorithms or dictionaries to handle this step.\n",
        "\n",
        "**Step 8: Tokenization**\n",
        "\n",
        "Tokenization involves breaking the text into individual words or tokens. This step is crucial for further analysis because it allows you to work with individual words, making it easier to analyze sentiment at a granular level.\n",
        "\n",
        "**Step 9: Remove Stop Words**\n",
        "\n",
        "Stop words are common words such as \"the,\" \"and,\" \"in,\" which occur frequently in the language but often don't carry much sentiment information. Removing stop words can reduce noise in the analysis and help focus on content words with more sentiment significance.\n",
        "\n",
        "**Step 10: Stemming and Lemmatization**\n",
        "\n",
        "Stemming and lemmatization are techniques to reduce words to their root or base forms. Stemming involves chopping off prefixes or suffixes to get to the root word (e.g., \"jumping\" becomes \"jump\"). Lemmatization, on the other hand, reduces words to their dictionary or base form (e.g., \"better\" becomes \"good\"). These techniques help ensure that variations of words are treated as the same word, improving analysis accuracy.\n",
        "\n",
        "**Step 11: Algorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ace886e4",
      "metadata": {
        "id": "ace886e4"
      },
      "source": [
        "#### Step 1: Remove HTML Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75f5c119",
      "metadata": {
        "id": "75f5c119"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def remove_html_tags(text):\n",
        "    pattern = re.compile('<.*?>')\n",
        "    return pattern.sub(r'', text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df5c891a",
      "metadata": {
        "id": "df5c891a"
      },
      "outputs": [],
      "source": [
        "text = \"<html><body><p> Movie 1</p><p> Actor - Aamir Khan</p><p> Click here to <a href='http://google.com'>download</a></p></body></html>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c711f7bb",
      "metadata": {
        "id": "c711f7bb"
      },
      "outputs": [],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85767682",
      "metadata": {
        "id": "85767682"
      },
      "outputs": [],
      "source": [
        "remove_html_tags(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e6b65bb",
      "metadata": {
        "id": "8e6b65bb"
      },
      "source": [
        "#### Step 2: Remove URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec50b9a1",
      "metadata": {
        "id": "ec50b9a1"
      },
      "outputs": [],
      "source": [
        "def remove_url(text):\n",
        "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return pattern.sub(r'', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1d2fde6",
      "metadata": {
        "id": "c1d2fde6"
      },
      "outputs": [],
      "source": [
        "text1 = 'Check out my notebook https://www.kaggle.com/ubaidshah/notebook8223fc1abb'\n",
        "text2 = 'Check out my notebook http://www.kaggle.com/ubaidshah/notebook8223fc1abb'\n",
        "text3 = 'Google search here www.google.com'\n",
        "text4 = 'For notebook click https://www.kaggle.com/ubaidshah/notebook8223fc1abb to search check www.google.com'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9e51d6a",
      "metadata": {
        "id": "a9e51d6a"
      },
      "outputs": [],
      "source": [
        "remove_url(text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8e497b",
      "metadata": {
        "id": "6f8e497b"
      },
      "outputs": [],
      "source": [
        "dataset['text'] = dataset['text'].apply(lambda x: remove_url(x))\n",
        "dataset['text'].tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "359c5c31",
      "metadata": {
        "id": "359c5c31"
      },
      "source": [
        "####  Step 3: Handling Emojis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acb78232",
      "metadata": {
        "id": "acb78232"
      },
      "outputs": [],
      "source": [
        "# Defining dictionary containing all emojis with their meanings.\n",
        "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad',\n",
        "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
        "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed',\n",
        "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
        "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
        "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink',\n",
        "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2ea5673",
      "metadata": {
        "id": "d2ea5673"
      },
      "outputs": [],
      "source": [
        "def handel_emoji(text):\n",
        "    for emoji in emojis.keys():\n",
        "        text = text.replace(emoji, \"EMOJI\" + emojis[emoji])\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25fa39b7",
      "metadata": {
        "id": "25fa39b7"
      },
      "outputs": [],
      "source": [
        "handel_emoji(\"@stustone Your show is whack. Way worse than whack, it's wiggety-whack.  :(:(:(\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "831e91a6",
      "metadata": {
        "id": "831e91a6"
      },
      "outputs": [],
      "source": [
        "dataset['text']=dataset['text'].apply(lambda x:handel_emoji(x) )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "601f9197",
      "metadata": {
        "id": "601f9197"
      },
      "source": [
        "#### Step 4: Chat word treatment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ade6867",
      "metadata": {
        "id": "3ade6867"
      },
      "outputs": [],
      "source": [
        "url1='https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt'\n",
        "slang='/content/drive/MyDrive/DATA SETS/slang.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bda11d17",
      "metadata": {
        "id": "bda11d17"
      },
      "outputs": [],
      "source": [
        "slang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41c43ba7",
      "metadata": {
        "id": "41c43ba7"
      },
      "outputs": [],
      "source": [
        "with open(slang,'r') as f:\n",
        "    lines = f.readlines()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9554c50d",
      "metadata": {
        "id": "9554c50d"
      },
      "outputs": [],
      "source": [
        "lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13beca88",
      "metadata": {
        "id": "13beca88"
      },
      "outputs": [],
      "source": [
        "\n",
        "(lines[0].split('='))[1][:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44031a44",
      "metadata": {
        "id": "44031a44"
      },
      "outputs": [],
      "source": [
        "chat_words=dict()\n",
        "for i in range(len(lines)):\n",
        "    chat_words[(lines[i].split('='))[0]]=(lines[i].split('='))[1][:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53f8b09a",
      "metadata": {
        "id": "53f8b09a"
      },
      "outputs": [],
      "source": [
        "chat_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddf6948d",
      "metadata": {
        "id": "ddf6948d"
      },
      "outputs": [],
      "source": [
        "def chat_conversion(text):\n",
        "    new_text = []\n",
        "    for w in text.split():\n",
        "        if w.upper() in chat_words:\n",
        "            new_text.append(chat_words[w.upper()])\n",
        "        else:\n",
        "            new_text.append(w)\n",
        "    return \" \".join(new_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd2a3ab9",
      "metadata": {
        "id": "cd2a3ab9"
      },
      "outputs": [],
      "source": [
        "dataset.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "885dd326",
      "metadata": {
        "id": "885dd326"
      },
      "outputs": [],
      "source": [
        "print(chat_conversion(dataset.iloc[7][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bdea2ef",
      "metadata": {
        "id": "1bdea2ef"
      },
      "outputs": [],
      "source": [
        "dataset['text'] = dataset['text'].apply(lambda x: chat_conversion(x))\n",
        "dataset['text'].tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf1369c4",
      "metadata": {
        "id": "cf1369c4"
      },
      "outputs": [],
      "source": [
        "print(dataset.iloc[7][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baa1179c",
      "metadata": {
        "id": "baa1179c"
      },
      "source": [
        "#### Step 5: Remove Punctiations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a87ac64a",
      "metadata": {
        "id": "a87ac64a"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1f4f896",
      "metadata": {
        "id": "d1f4f896"
      },
      "outputs": [],
      "source": [
        "exclude = string.punctuation\n",
        "def remove_punc(text):\n",
        "    return text.translate(str.maketrans('', '', exclude))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c88fa5f",
      "metadata": {
        "id": "1c88fa5f"
      },
      "outputs": [],
      "source": [
        "text = 'string. With. Punctuation?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "532a3b38",
      "metadata": {
        "id": "532a3b38"
      },
      "outputs": [],
      "source": [
        "remove_punc(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c171d0",
      "metadata": {
        "id": "23c171d0"
      },
      "outputs": [],
      "source": [
        "dataset['text'] = dataset['text'].apply(lambda x: remove_punc(x))\n",
        "dataset['text'].tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7192c9f7",
      "metadata": {
        "id": "7192c9f7"
      },
      "source": [
        "#### STEP 6: Make Lower Case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bfed585",
      "metadata": {
        "id": "3bfed585"
      },
      "outputs": [],
      "source": [
        "dataset['text']=dataset['text'].str.lower()\n",
        "dataset['text'].tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "959e62c7",
      "metadata": {
        "id": "959e62c7"
      },
      "source": [
        "#### Step 7: Spelling Correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13ba9559",
      "metadata": {
        "id": "13ba9559"
      },
      "outputs": [],
      "source": [
        "# ! pip install textblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54a09a15",
      "metadata": {
        "scrolled": true,
        "id": "54a09a15"
      },
      "outputs": [],
      "source": [
        "# from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e8a0b8",
      "metadata": {
        "id": "71e8a0b8"
      },
      "outputs": [],
      "source": [
        "# incorrect_text = 'ceertain conditionas duriing seveal ggenerations aree moodified in the saame maner.'\n",
        "\n",
        "# textBlb = TextBlob(incorrect_text)\n",
        "# str(textBlb.correct())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f841e1c4",
      "metadata": {
        "id": "f841e1c4"
      },
      "outputs": [],
      "source": [
        "# incorrect_text = \"ahh i've always wanted to see rent  love the soundtrack!!\"\n",
        "\n",
        "# textBlb = TextBlob(incorrect_text)\n",
        "# str(textBlb.correct())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72e6ae48",
      "metadata": {
        "id": "72e6ae48"
      },
      "outputs": [],
      "source": [
        "# def spell_correct(text):\n",
        "#     # return TextBlob(text).correct().string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f925b019",
      "metadata": {
        "id": "f925b019"
      },
      "outputs": [],
      "source": [
        "# print(spell_correct('ahh ive always wanted to see rent  love the soundtrack!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24b1c893",
      "metadata": {
        "id": "24b1c893"
      },
      "outputs": [],
      "source": [
        "# dataset['text']=dataset['text'].apply(lambda x:spell_correct(x) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae140991",
      "metadata": {
        "id": "ae140991"
      },
      "outputs": [],
      "source": [
        "# import itertools\n",
        "# from autocorrect import Speller\n",
        "# text=\"ahh ive always wanted to see rent  love the soundtrack!!\"\n",
        "# # #One letter in a word should not be present more than twice in continuation\n",
        "# # text_correction = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
        "# # print(\"Normal Text:\\n{}\".format(text_correction))\n",
        "# spell = Speller(lang='en')\n",
        "# ans = spell(text)\n",
        "# print(\"After correcting text:\\n{}\".format(ans))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "812ac6e7",
      "metadata": {
        "id": "812ac6e7"
      },
      "outputs": [],
      "source": [
        "# def auto_correct(text):\n",
        "#     spell=Speller(lang='en')\n",
        "#     return spell(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54cdbbc3",
      "metadata": {
        "id": "54cdbbc3"
      },
      "outputs": [],
      "source": [
        "# dataset['text']=dataset['text'].apply(lambda x:auto_correct(x) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "242495cb",
      "metadata": {
        "id": "242495cb"
      },
      "outputs": [],
      "source": [
        "# dataset.iloc[201][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a07536c0",
      "metadata": {
        "id": "a07536c0"
      },
      "outputs": [],
      "source": [
        "# import spacy\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "# text = 'My email is abc@gmail.com'\n",
        "# doc = nlp(text)\n",
        "# l=[]\n",
        "# for token in doc:\n",
        "# #     print(token)\n",
        "#     if not token.like_email:\n",
        "# #         l.append(str(token))\n",
        "# \" \".join(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dbd09d8",
      "metadata": {
        "id": "9dbd09d8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60256a53",
      "metadata": {
        "id": "60256a53"
      },
      "outputs": [],
      "source": [
        "# !pip install autocorrect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f61aa113",
      "metadata": {
        "id": "f61aa113"
      },
      "outputs": [],
      "source": [
        "# ! pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff6f2f5c",
      "metadata": {
        "id": "ff6f2f5c"
      },
      "outputs": [],
      "source": [
        "# ! python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ea0910b",
      "metadata": {
        "id": "6ea0910b"
      },
      "source": [
        "#### Step 8: Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "154f09ec",
      "metadata": {
        "id": "154f09ec"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58233c7e",
      "metadata": {
        "id": "58233c7e"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize,sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d751baf",
      "metadata": {
        "id": "3d751baf"
      },
      "outputs": [],
      "source": [
        "sent_tokenize(\"@stustone Your show is whack. Way worse than whack, it's wiggety-whack.  EMOJIsadEMOJIsadEMOJIsad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c8f548a",
      "metadata": {
        "id": "1c8f548a"
      },
      "outputs": [],
      "source": [
        "sent=sent_tokenize(\"@stustone Your show is whack. Way worse than whack, it's wiggety-whack.  EMOJIsadEMOJIsadEMOJIsad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1301fcc",
      "metadata": {
        "id": "a1301fcc"
      },
      "outputs": [],
      "source": [
        "wt=word_tokenize((\"@stustone Your show is whack. Way worse than whack, it's wiggety-whack.  EMOJIsadEMOJIsadEMOJIsad\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wt"
      ],
      "metadata": {
        "id": "1yXG-WNBZuxK"
      },
      "id": "1yXG-WNBZuxK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ea1717",
      "metadata": {
        "id": "e9ea1717"
      },
      "outputs": [],
      "source": [
        "def word_tokenize(text):\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 9: Remove Stop Words"
      ],
      "metadata": {
        "id": "UKKKzS34fzOi"
      },
      "id": "UKKKzS34fzOi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d252e9a",
      "metadata": {
        "id": "7d252e9a"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a52ee08",
      "metadata": {
        "id": "2a52ee08"
      },
      "outputs": [],
      "source": [
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56ada84a",
      "metadata": {
        "id": "56ada84a"
      },
      "outputs": [],
      "source": [
        "print(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48079235",
      "metadata": {
        "id": "48079235"
      },
      "outputs": [],
      "source": [
        "sample_words = [word for word in wt if word not in stopwords.words('english')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "725cfdf8",
      "metadata": {
        "id": "725cfdf8"
      },
      "outputs": [],
      "source": [
        "print(\" \".join(sample_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c7404ed",
      "metadata": {
        "id": "3c7404ed"
      },
      "outputs": [],
      "source": [
        "\" \".join(wt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3feb9033",
      "metadata": {
        "id": "3feb9033"
      },
      "outputs": [],
      "source": [
        "# import spacy\n",
        "# nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cceaa0c",
      "metadata": {
        "id": "2cceaa0c"
      },
      "outputs": [],
      "source": [
        "# doc1 = nlp(\"@stustone Your show is whack. Way worse than whack, it's wiggety-whack.  EMOJIsadEMOJIsadEMOJIsad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb2469f3",
      "metadata": {
        "id": "fb2469f3"
      },
      "outputs": [],
      "source": [
        "# for i in doc1:\n",
        "#     print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab2a1505",
      "metadata": {
        "id": "ab2a1505"
      },
      "outputs": [],
      "source": [
        "# w=(\"@stustone Your show is whack. Way worse than whack, it's wiggety-whack.  EMOJIsadEMOJIsadEMOJIsad\").split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2bb82de",
      "metadata": {
        "id": "f2bb82de"
      },
      "outputs": [],
      "source": [
        "# sample_words = [word for word in w if word not in stopwords.words('english')]\n",
        "# print(\" \".join(sample_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3b88d38",
      "metadata": {
        "id": "f3b88d38"
      },
      "outputs": [],
      "source": [
        "def token_split(text):\n",
        "    lis_w=text.split()\n",
        "    return lis_w\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9561426",
      "metadata": {
        "id": "b9561426"
      },
      "outputs": [],
      "source": [
        "dataset['text']=dataset['text'].apply(lambda x:token_split(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4773e0b2",
      "metadata": {
        "id": "4773e0b2"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bc1486f",
      "metadata": {
        "id": "5bc1486f"
      },
      "outputs": [],
      "source": [
        "# def stopword_remove(lis):\n",
        "#     sample_words = [word for word in lis if word not in stopwords.words('english')]\n",
        "#     return \" \".join(sample_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5fed252",
      "metadata": {
        "id": "a5fed252"
      },
      "outputs": [],
      "source": [
        "# dataset['text']=dataset['text'].apply(lambda x:stopword_remove(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f985286a",
      "metadata": {
        "id": "f985286a"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d720dfb",
      "metadata": {
        "id": "2d720dfb"
      },
      "source": [
        "#### Step 10: Stemming and Lemmitization\n",
        "![Imgur](https://i.imgur.com/uqNdwzX.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f022cd3d",
      "metadata": {
        "id": "f022cd3d"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "st=PorterStemmer()\n",
        "def stemming_on_text(data):\n",
        "    text = [st.stem(word) for word in data]\n",
        "    return \" \".join(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['text'] = dataset['text'].apply(lambda x: stemming_on_text(x))\n",
        "dataset['text'].head()"
      ],
      "metadata": {
        "id": "2tFX3Ps4wfHd"
      },
      "id": "2tFX3Ps4wfHd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90e2cdf8",
      "metadata": {
        "id": "90e2cdf8"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a11ee667",
      "metadata": {
        "id": "a11ee667"
      },
      "outputs": [],
      "source": [
        "lm = WordNetLemmatizer()\n",
        "def lemmatizer_on_text(text):\n",
        "    data = [lm.lemmatize(word,pos='v') for word in text.split()]\n",
        "    return \" \".join(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4e69dbc",
      "metadata": {
        "id": "b4e69dbc"
      },
      "outputs": [],
      "source": [
        "lemmatizer_on_text((dataset['text'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e14c93a",
      "metadata": {
        "id": "8e14c93a"
      },
      "outputs": [],
      "source": [
        "dataset['text'] = dataset['text'].apply(lambda x: lemmatizer_on_text(x))\n",
        "dataset['text'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f89cb415",
      "metadata": {
        "scrolled": true,
        "id": "f89cb415"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a0aa05f",
      "metadata": {
        "id": "1a0aa05f"
      },
      "outputs": [],
      "source": [
        "dataset['target'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.to_csv('/content/drive/MyDrive/DATA SETS/processed_tweets.csv',index=False)"
      ],
      "metadata": {
        "id": "sykNrRyG_tHI"
      },
      "id": "sykNrRyG_tHI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 11: Apply Algorithm"
      ],
      "metadata": {
        "id": "RHkbKBLrgAcp"
      },
      "id": "RHkbKBLrgAcp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF**, which stands for **Term Frequency-Inverse Document Frequency**, is a numerical statistic used in natural language processing and information retrieval to evaluate the importance of a term (word) within a document relative to a collection of documents (corpus). It's a common technique for text feature extraction and is particularly useful for text-based applications like information retrieval, text classification, and document ranking.\n",
        "\n",
        "The formula for calculating TF-IDF for a term in a document is as follows:\n",
        "\n",
        "**TF-IDF(t, d) = TF(t, d) * IDF(t)**\n",
        "\n",
        "Where:\n",
        "\n",
        "TF(t, d) (Term Frequency): This component measures the frequency of a term (t) within a specific document (d). It indicates how often the term appears in the document and can be computed in various ways, such as simple word count or normalized frequency (e.g., by dividing the raw count by the total number of words in the document). A common formula for TF is:\n",
        "\n",
        "**TF(t, d) = (Number of times term t appears in document d) / (Total number of terms in document d)**\n",
        "\n",
        "IDF(t) (Inverse Document Frequency): This component assesses the importance of a term across a collection of documents. It's calculated as:\n",
        "\n",
        "**IDF(t) = log(N / (n_t + 1))**\n",
        "\n",
        "Where:\n",
        "\n",
        "N is the total number of documents in the corpus.\n",
        "n_t is the number of documents that contain the term t.\n",
        "The \"+1\" in the denominator is a smoothing factor to avoid division by zero when a term is not found in any documents in the corpus."
      ],
      "metadata": {
        "id": "F_FrzZghkO0n"
      },
      "id": "F_FrzZghkO0n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78de841b",
      "metadata": {
        "id": "78de841b"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset['text'], dataset['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=500000,ngram_range=(1,3),stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d4fb081",
      "metadata": {
        "id": "8d4fb081"
      },
      "outputs": [],
      "source": [
        "X_train_tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5380e995",
      "metadata": {
        "scrolled": true,
        "id": "5380e995"
      },
      "outputs": [],
      "source": [
        "X_train_tfidf.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8323a418",
      "metadata": {
        "id": "8323a418"
      },
      "outputs": [],
      "source": [
        "print(X_train_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfb0d659",
      "metadata": {
        "id": "cfb0d659"
      },
      "outputs": [],
      "source": [
        "print(\"Feature Names n\",tfidf_vectorizer.get_feature_names_out())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b8016a2",
      "metadata": {
        "id": "6b8016a2"
      },
      "outputs": [],
      "source": [
        "for i, feature in enumerate(tfidf_vectorizer.get_feature_names_out()):\n",
        "    print(i, feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e52cb0",
      "metadata": {
        "id": "71e52cb0"
      },
      "outputs": [],
      "source": [
        "# X_train_tfidf.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes are three different variants of the Naive Bayes algorithm, each suited for specific types of data and classification tasks. Here's a comparison of these variants:\n",
        "\n",
        "1. **Gaussian Naive Bayes:**\n",
        "\n",
        "   - **Use Case:** It is suitable for continuous data that follows a Gaussian (normal) distribution. It is commonly used for real-valued features.\n",
        "\n",
        "   - **Mathematics:** It models the likelihood of features as Gaussian distributions. It assumes that the features are conditionally independent given the class.\n",
        "\n",
        "   - **Strengths:**\n",
        "     - Effective for continuous, real-valued data.\n",
        "     - Works well for data that can be reasonably approximated by a Gaussian distribution.\n",
        "\n",
        "   - **Weaknesses:**\n",
        "     - May not perform well for data with non-Gaussian distributions.\n",
        "     - Not ideal for text data or data with a large number of discrete categories.\n",
        "\n",
        "   - **Example Use Cases:** Handwriting recognition, facial recognition, medical data analysis.\n",
        "\n",
        "2. **Multinomial Naive Bayes:**\n",
        "\n",
        "   - **Use Case:** It is suitable for discrete data, especially when dealing with text data. It is commonly used in text classification tasks.\n",
        "\n",
        "   - **Mathematics:** It models the likelihood of features as a Multinomial distribution, assuming features represent word frequencies (e.g., using TF-IDF values).\n",
        "\n",
        "   - **Strengths:**\n",
        "     - Effective for text classification tasks, such as sentiment analysis and spam detection.\n",
        "     - Handles discrete data well, where each feature represents the count or frequency of a term.\n",
        "\n",
        "   - **Weaknesses:**\n",
        "     - It assumes features are categorical and independent, which may not hold in some cases.\n",
        "     - Ignores the order of words in a document.\n",
        "\n",
        "   - **Example Use Cases:** Sentiment analysis, document classification, spam email detection.\n",
        "\n",
        "3. **Bernoulli Naive Bayes:**\n",
        "\n",
        "   - **Use Case:** It is suitable for binary data, where features represent binary attributes (0/1 values). It is often used in document classification tasks where you have binary presence/absence features.\n",
        "\n",
        "   - **Mathematics:** It models the likelihood of features as a Bernoulli distribution.\n",
        "\n",
        "   - **Strengths:**\n",
        "     - Effective for binary data, where features are binary indicators (e.g., word presence/absence).\n",
        "     - Suitable for tasks like spam detection and document classification.\n",
        "\n",
        "   - **Weaknesses:**\n",
        "     - Ignores term frequency information (only considers binary presence/absence).\n",
        "     - Assumes independence of features.\n",
        "\n",
        "   - **Example Use Cases:** Text classification with binary features, such as spam detection or sentiment analysis with a bag-of-words representation.\n",
        "\n",
        "In summary, the choice between Gaussian, Multinomial, and Bernoulli Naive Bayes depends on the nature of your data and the specific classification task you're working on. For text classification, Multinomial and Bernoulli Naive Bayes are often more appropriate, while Gaussian Naive Bayes is better suited for continuous data. The effectiveness of each variant depends on the appropriateness of its underlying distribution assumption to the data you're working with."
      ],
      "metadata": {
        "id": "yvNgqZXMU5jH"
      },
      "id": "yvNgqZXMU5jH"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DC5kAitYU4vo"
      },
      "id": "DC5kAitYU4vo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "419af2b1",
      "metadata": {
        "id": "419af2b1"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "y_pred = nb_model.predict(X_test_tfidf)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_tweets = [\"I love this product!\", \"This is terrible.\"]\n",
        "new_tweets_tfidf = tfidf_vectorizer.transform(new_tweets)\n",
        "\n",
        "nb_sentiments = nb_model.predict(new_tweets_tfidf)\n",
        "\n",
        "print(\"Sentiments (Naive Bayes):\", nb_sentiments)\n"
      ],
      "metadata": {
        "id": "nV94gjfh706C"
      },
      "id": "nV94gjfh706C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet=['''Agencies slipping in my DM on behalf of T Series and Adipurush and begging me to delete my tweets for some money, sorry guys you chose the wrong person. #AdipurushDisaster ''']"
      ],
      "metadata": {
        "id": "BFrNZTerTvqw"
      },
      "id": "BFrNZTerTvqw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_tweets_tfidf = tfidf_vectorizer.transform(tweet)\n",
        "\n",
        "nb_sentiments = nb_model.predict(new_tweets_tfidf)\n",
        "\n",
        "print(\"Sentiments (Naive Bayes):\", nb_sentiments)"
      ],
      "metadata": {
        "id": "pyCpPi-_T26t"
      },
      "id": "pyCpPi-_T26t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ak4BlAaTT83X"
      },
      "id": "Ak4BlAaTT83X",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ace886e4",
        "8e6b65bb",
        "359c5c31",
        "601f9197",
        "baa1179c",
        "7192c9f7",
        "959e62c7",
        "6ea0910b",
        "UKKKzS34fzOi"
      ]
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}